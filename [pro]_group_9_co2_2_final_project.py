# -*- coding: utf-8 -*-
"""[PRO] Group 9_CO2.2 Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UAbZWlVsMzELf6hhiQlhMIFp1Jafxk-Z
"""

!pip install kagglehub torch torchvision torchaudio rfdetr

import kagglehub
import os
import shutil
import json
from PIL import Image
import torch
from rfdetr import RFDETRBase

# Download latest version
path = kagglehub.dataset_download("banuprasadb/visdrone-dataset")

print("Path to dataset files:", path)

import datetime

def copy_and_resize_images(src_dir, dst_dir, resize_to=(512, 512)):
    os.makedirs(dst_dir, exist_ok=True)
    for file in os.listdir(src_dir):
        if file.endswith('.jpg'):
            src_path = os.path.join(src_dir, file)
            dst_path = os.path.join(dst_dir, file)
            try:
                with Image.open(src_path) as img:
                    resized_img = img.resize(resize_to)
                    resized_img.save(dst_path)
            except Exception as e:
                print(f"Error processing image {file}: {e}")

def convert_yolo_to_coco(images_dir, labels_dir, output_json_path):
    coco_format = {
        "info": {
            "description": "VisDrone 2019-DET Dataset in COCO format",
            "version": "1.0",
            "year": 2019,
            "contributor": "RF-DETR conversion",
            "date_created": datetime.datetime.utcnow().isoformat(' ')
        },
        "images": [],
        "annotations": [],
        "categories": []
    }

    categories = [
        {"id": 0, "name": "pedestrian", "supercategory": "person"},
        {"id": 1, "name": "person", "supercategory": "person"},
        {"id": 2, "name": "bicycle", "supercategory": "vehicle"},
        {"id": 3, "name": "car", "supercategory": "vehicle"},
        {"id": 4, "name": "van", "supercategory": "vehicle"},
        {"id": 5, "name": "truck", "supercategory": "vehicle"},
        {"id": 6, "name": "tricycle", "supercategory": "vehicle"},
        {"id": 7, "name": "awning-tricycle", "supercategory": "vehicle"},
        {"id": 8, "name": "bus", "supercategory": "vehicle"},
        {"id": 9, "name": "motor", "supercategory": "vehicle"}
    ]
    coco_format["categories"] = categories

    image_id = 0
    annotation_id = 0

    for label_file in os.listdir(labels_dir):
        if label_file.endswith(".txt"):
            image_file = os.path.splitext(label_file)[0] + ".jpg"
            image_path = os.path.join(images_dir, image_file)

            if not os.path.exists(image_path):
                print(f"Warning: Image not found for label file {label_file}")
                continue

            try:
                with Image.open(image_path) as img:
                    width, height = img.size
            except Exception as e:
                print(f"Error opening image {image_file}: {e}")
                continue

            coco_format["images"].append({
                "id": image_id,
                "file_name": image_file,
                "width": width,
                "height": height
            })

            with open(os.path.join(labels_dir, label_file), "r") as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 5:
                        class_id, center_x, center_y, bbox_width, bbox_height = map(float, parts)

                        x_center = center_x * width
                        y_center = center_y * height
                        w = bbox_width * width
                        h = bbox_height * height

                        x_min = x_center - w / 2
                        y_min = y_center - h / 2

                        if int(class_id) >= 0 and int(class_id) < 10:
                            coco_format["annotations"].append({
                                "id": annotation_id,
                                "image_id": image_id,
                                "category_id": int(class_id),
                                "bbox": [x_min, y_min, w, h],
                                "area": w * h,
                                "iscrowd": 0
                            })
                            annotation_id += 1
                        else:
                            print(f"Ignoring class_id {int(class_id)} in {label_file}")

            image_id += 1

    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
    with open(output_json_path, "w") as f:
        json.dump(coco_format, f, indent=4)
    print(f"COCO annotations saved to {output_json_path}")

# Define source and target paths
src_base = os.path.join(path, "VisDrone_Dataset")
target_base = "/content/rf-detr/data/visdrone"

# Define input directories for images and labels
train_img_in = os.path.join(src_base, "VisDrone2019-DET-train/images")
val_img_in = os.path.join(src_base, "VisDrone2019-DET-val/images")
test_img_in = os.path.join(src_base, "VisDrone2019-DET-test-dev/images")

train_lbl_in = os.path.join(src_base, "VisDrone2019-DET-train/labels")
val_lbl_in = os.path.join(src_base, "VisDrone2019-DET-val/labels")
test_lbl_in = os.path.join(src_base, "VisDrone2019-DET-test-dev/labels")

# Define the final target directories for images and annotations
train_img_out = os.path.join(target_base, "train")
val_img_out = os.path.join(target_base, "val")
valid_img_out = os.path.join(target_base, "valid")
test_img_out = os.path.join(target_base, "test")

train_json_out = os.path.join(train_img_out, "_annotations.coco.json")
val_json_out = os.path.join(val_img_out, "_annotations.coco.json")
valid_json_out = os.path.join(valid_img_out, "_annotations.coco.json")
test_json_out = os.path.join(test_img_out, "_annotations.coco.json")

# Create the necessary target directories
os.makedirs(train_img_out, exist_ok=True)
os.makedirs(val_img_out, exist_ok=True)
os.makedirs(valid_img_out, exist_ok=True)
os.makedirs(test_img_out, exist_ok=True)

# Copy and resize images to the target
copy_and_resize_images(train_img_in, train_img_out)
copy_and_resize_images(val_img_in, val_img_out)
copy_and_resize_images(val_img_in, valid_img_out)
copy_and_resize_images(test_img_in, test_img_out)

# Convert YOLO annotations to COCO format
convert_yolo_to_coco(train_img_out, train_lbl_in, train_json_out)
convert_yolo_to_coco(val_img_out, val_lbl_in, val_json_out)
convert_yolo_to_coco(valid_img_out, val_lbl_in, valid_json_out)
convert_yolo_to_coco(test_img_out, test_lbl_in, test_json_out)

print("Data preparation complete.")

!pip install tensorboard

!pip install "rfdetr[metrics]"

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir /content/rf-detr/output

os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

model = RFDETRBase(num_classes=10)

model.train(
    dataset_dir="/content/rf-detr/data/visdrone",
    epochs=10,
    batch_size=6,
    grad_accum_steps=2,
    lr=1e-4,
    output_dir="/content/rf-detr/output",
    tensorboard=True,
)

#import torch
#torch.cuda.empty_cache()
#torch.cuda.ipc_collect()

"""**PAUSE**"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir /content/rf-detr/output

"""**CHECKPOINT**


We included this part to avoid data loss or progress loss, so that we can have a clear idea as to where we left off before adding more epochs..
"""

import torch
torch.cuda.empty_cache()
torch.cuda.ipc_collect()

model.train(
    dataset_dir="/content/rf-detr/data/visdrone",
    epochs=25,
    batch_size=6,
    grad_accum_steps=2,
    lr=1e-4,
    output_dir="/content/rf-detr/output",
    resume="/content/rf-detr/output/checkpoint.pth",
    tensorboard=True,
    early_stopping=True,
    early_stopping_patience=5,
    early_stopping_min_delta=0.001,
    early_stopping_use_ema=False
)

"""**CHECKPOINT 2**"""

import torch
torch.cuda.empty_cache()
torch.cuda.ipc_collect()

model.train(
    dataset_dir="/content/rf-detr/data/visdrone",
    epochs=30,
    batch_size=6,
    grad_accum_steps=2,
    lr=5e-5,
    output_dir="/content/rf-detr/output",
    tensorboard=True,
    resume="/content/rf-detr/output/checkpoint.pth"
)

model.train(
    dataset_dir="/content/rf-detr/data/visdrone",
    epochs=30,
    batch_size=6,
    grad_accum_steps=2,
    lr=5e-5,
    output_dir="/content/rf-detr/output",
    tensorboard=True,
    resume="/content/rf-detr/output/checkpoint.pth",
    start_epoch=25
)

"""METRICS"""

!pip install pycocotools
!pip install -q roboflow rfdetr

import os
from rfdetr import RFDETRBase
from PIL import Image
import matplotlib.pyplot as plt

!pip install kagglehub torch torchvision torchaudio

import kagglehub
import shutil
import json
import torch

# Download latest version
path = kagglehub.dataset_download("banuprasadb/visdrone-dataset")

print("Path to dataset files:", path)

import os
import json
import datetime
from PIL import Image
import shutil

def copy_and_resize_images(src_dir, dst_dir):
    os.makedirs(dst_dir, exist_ok=True)
    for file in os.listdir(src_dir):
        if file.endswith('.jpg'):
            src_path = os.path.join(src_dir, file)
            dst_path = os.path.join(dst_dir, file)
            try:
                shutil.copy(src_path, dst_path)
            except Exception as e:
                print(f"Error processing image {file}: {e}")


def convert_yolo_to_coco(images_dir, labels_dir, output_json_path):
    coco_format = {
        "info": {
            "description": "VisDrone 2019-DET Dataset in COCO format",
            "version": "1.0",
            "year": 2019,
            "contributor": "RF-DETR conversion",
            "date_created": datetime.datetime.utcnow().isoformat(' ')
        },
        "images": [],
        "annotations": [],
        "categories": []
    }

    categories = [
        {"id": 0, "name": "pedestrian", "supercategory": "person"},
        {"id": 1, "name": "person", "supercategory": "person"},
        {"id": 2, "name": "bicycle", "supercategory": "vehicle"},
        {"id": 3, "name": "car", "supercategory": "vehicle"},
        {"id": 4, "name": "van", "supercategory": "vehicle"},
        {"id": 5, "name": "truck", "supercategory": "vehicle"},
        {"id": 6, "name": "tricycle", "supercategory": "vehicle"},
        {"id": 7, "name": "awning-tricycle", "supercategory": "vehicle"},
        {"id": 8, "name": "bus", "supercategory": "vehicle"},
        {"id": 9, "name": "motor", "supercategory": "vehicle"}
    ]
    coco_format["categories"] = categories

    image_id = 0
    annotation_id = 0

    for label_file in os.listdir(labels_dir):
        if label_file.endswith(".txt"):
            image_file = os.path.splitext(label_file)[0] + ".jpg"
            image_path = os.path.join(images_dir, image_file)

            if not os.path.exists(image_path):
                print(f"Warning: Image not found for label file {label_file}")
                continue

            try:
                with Image.open(image_path) as img:
                    width, height = img.size
            except Exception as e:
                print(f"Error opening image {image_file}: {e}")
                continue


            coco_format["images"].append({
                "id": image_id,
                "file_name": image_file,
                "width": width,
                "height": height
            })

            with open(os.path.join(labels_dir, label_file), "r") as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) == 5:
                        class_id, center_x, center_y, bbox_width, bbox_height = map(float, parts)

                        x_center = center_x * width
                        y_center = center_y * height
                        w = bbox_width * width
                        h = bbox_height * height

                        x_min = x_center - w / 2
                        y_min = y_center - h / 2

                        if int(class_id) >= 0 and int(class_id) < 10:
                            coco_format["annotations"].append({
                                "id": annotation_id,
                                "image_id": image_id,
                                "category_id": int(class_id),
                                "bbox": [x_min, y_min, w, h],
                                "area": w * h,
                                "iscrowd": 0
                            })
                            annotation_id += 1
                        else:
                            print(f"Ignoring class_id {int(class_id)} in {label_file}")


            image_id += 1

    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
    with open(output_json_path, "w") as f:
        json.dump(coco_format, f, indent=4)
    print(f"COCO annotations saved to {output_json_path}")


# Set base paths
src_base = os.path.join(path, "VisDrone_Dataset")
target_base = "/content/rf-detr/data/visdrone"

# Input image and label directories
train_img_in = os.path.join(src_base, "VisDrone2019-DET-train/images")
val_img_in = os.path.join(src_base, "VisDrone2019-DET-val/images")
test_img_in = os.path.join(src_base, "VisDrone2019-DET-test-dev/images")

train_lbl_in = os.path.join(src_base, "VisDrone2019-DET-train/labels")
val_lbl_in = os.path.join(src_base, "VisDrone2019-DET-val/labels")
test_lbl_in = os.path.join(src_base, "VisDrone2019-DET-test-dev/labels")


# Define the final target directories for images and annotations within rf-detr's data structure
train_img_out = os.path.join(target_base, "train")
val_img_out = os.path.join(target_base, "val")
valid_img_out = os.path.join(target_base, "valid")
test_img_out = os.path.join(target_base, "test")

train_json_out = os.path.join(train_img_out, "_annotations.coco.json")
val_json_out = os.path.join(val_img_out, "_annotations.coco.json")
valid_json_out = os.path.join(valid_img_out, "_annotations.coco.json")
test_json_out = os.path.join(test_img_out, "_annotations.coco.json")

# Create the necessary target directories
os.makedirs(train_img_out, exist_ok=True)
os.makedirs(val_img_out, exist_ok=True)
os.makedirs(valid_img_out, exist_ok=True)
os.makedirs(test_img_out, exist_ok=True)

# Copy images to the target directories
copy_and_resize_images(train_img_in, train_img_out)
copy_and_resize_images(val_img_in, val_img_out)
copy_and_resize_images(val_img_in, valid_img_out)
copy_and_resize_images(test_img_in, test_img_out)


# Convert YOLO annotations to COCO format
convert_yolo_to_coco(train_img_out, train_lbl_in, train_json_out)
convert_yolo_to_coco(val_img_out, val_lbl_in, val_json_out)
convert_yolo_to_coco(valid_img_out, val_lbl_in, valid_json_out)
convert_yolo_to_coco(test_img_out, test_lbl_in, test_json_out)


print("Data preparation complete.")

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/rf-detr/output

!cp /content/drive/MyDrive/checkpoint_best_ema.pth /content/rf-detr/output/

import torch
import torchvision.transforms as T
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from pathlib import Path

from rfdetr import RFDETRBase

# Path to trained checkpoint
ckpt_path = "/content/rf-detr/output/checkpoint_best_ema.pth"

# Initialize RF-DETR model using the base variant
model = RFDETRBase(pretrain_weights=ckpt_path)
model.optimize_for_inference(batch_size=1)

import glob
import numpy as np
import torch
import torchvision.transforms as T
import cv2
import matplotlib.pyplot as plt
from PIL import Image
from pathlib import Path

from rfdetr import RFDETRBase

# Path to trained checkpoint
ckpt_path = "/content/rf-detr/output/checkpoint_best_ema.pth"

# Path to test images
test_dir = "/content/rf-detr/data/visdrone/test"
image_paths = sorted(glob.glob(f"{test_dir}/*.jpg"))[:10]

# Class names for visualization
class_names = [
    "pedestrian", "person", "bicycle", "car", "van",
    "truck", "tricycle", "awning-tricycle", "bus", "motor"
]

try:
    model
except NameError:
    print("Model not found. Initializing RFDETRBase model...")
    model = RFDETRBase(pretrain_weights=ckpt_path)


# Run inference
for image_path in image_paths:
    try:
        print(f"Processing image: {image_path}")
        img = Image.open(image_path)
        print(f"  After Image.open - Mode: {img.mode}, Bands: {img.getbands()}")

        # Convert to RGB
        img = img.convert("RGB")
        print(f"  After convert('RGB') - Mode: {img.mode}, Bands: {img.getbands()}")

        # Check the number of channels after conversion
        if img.mode != 'RGB':
            print(f"Warning: Image {image_path} could not be converted to RGB. Skipping.")
            continue

        orig_w, orig_h = img.size

        # Resize image
        resized_img = img.resize((560, 560))
        print(f"  After resize - Mode: {resized_img.mode}, Bands: {resized_img.getbands()}")

        # Convert PIL image to NumPy array and then to PyTorch tensor
        img_np = np.array(resized_img).astype(np.float32) / 255.0
        print(f"  After np.array - Shape: {img_np.shape}, Dtype: {img_np.dtype}")

        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0).cuda()
        print(f"  After torch.from_numpy and permute - Shape: {img_tensor.shape}, Dtype: {img_tensor.dtype}")


        with torch.no_grad():

          outputs = model.predict(img_tensor.squeeze(0))

        predictions = outputs

        # Convert attributes to tensors
        boxes = torch.tensor(predictions.xyxy).cuda()
        labels = torch.tensor(predictions.class_id).cuda()
        scores = torch.tensor(predictions.confidence).cuda()

        # Filter low-confidence predictions
        conf_thresh = 0.3
        keep = scores > conf_thresh
        boxes = boxes[keep]
        labels = labels[keep]
        scores = scores[keep]

        boxes_xyxy = boxes

        # Scale boxes
        scale_x = orig_w / 560
        scale_y = orig_h / 560
        boxes_xyxy[:, [0, 2]] *= scale_x
        boxes_xyxy[:, [1, 3]] *= scale_y

        # Draw boxes
        img_cv = np.array(img)
        for box, label, score in zip(boxes_xyxy, labels, scores):
            x0, y0, x1, y1 = box.int().tolist()
            if 0 <= label < len(class_names):
                class_name = class_names[label]
            else:
                class_name = f"Unknown ({label})"

            color = (0, 255, 0)
            cv2.rectangle(img_cv, (x0, y0), (x1, y1), color, 2)
            cv2.putText(img_cv, f"{class_name} {score:.2f}", (x0, y0 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)

        # Show image
        plt.figure(figsize=(10, 8))
        plt.imshow(img_cv)
        plt.title(f"Inference: {Path(image_path).name}")
        plt.axis("off")
        plt.show()

    except Exception as e:
        print(f"Error processing image {image_path}: {e}")
        continue

import os

log_dir = "/content/drive/MyDrive/tfevents"
os.listdir(log_dir)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
!rm -rf /tmp/tb_logs
!mkdir -p /tmp/tb_logs
!cp -r /content/drive/MyDrive/tfevents/* /tmp/tb_logs/

# Commented out IPython magic to ensure Python compatibility.
# Start TensorBoard with local copy
# %tensorboard --logdir /tmp/tb_logs

import os
from pathlib import Path

val_img_dir = "/content/rf-detr/data/visdrone/valid/"
val_anno_path = "/content/rf-detr/data/visdrone/valid/_annotations.coco.json"
output_json_path = "/content/rf-detr/output/predictions.json"
model_path = "/content/rf-detr/output/checkpoint_best_ema.pth"

import os
import json
import torch
from tqdm import tqdm
from PIL import Image
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from pathlib import Path
import torchvision.transforms as T

from rfdetr import RFDETRBase

# Path to test images and annotations
test_images_dir = "/content/rf-detr/data/visdrone/val"
gt_path = "/content/rf-detr/data/visdrone/val/_annotations.coco.json"

# Load model
detector = RFDETRBase(pretrain_weights="/content/rf-detr/output/checkpoint_best_ema.pth")
model = detector.model

# Transform for resizing and normalizing
transform = T.Compose([
    T.Resize((560, 560)),
    T.ToTensor(),
    T.Normalize(mean=detector.means, std=detector.stds)
])

import os

test_images_dir = "/content/rf-detr/data/visdrone/val"
actual_files = sorted(os.listdir(test_images_dir))
print(actual_files[:10])  # print sample

from pycocotools.coco import COCO

gt_path = "/content/rf-detr/data/visdrone/val/_annotations.coco.json"
coco = COCO(gt_path)

image_info = coco.loadImgs(coco.getImgIds())[0]
print("Expected file name:", image_info['file_name'])

import os
from PIL import Image
from tqdm import tqdm
from pycocotools.coco import COCO
import torchvision.transforms as T
import torch

# Define transform
transform = T.Compose([
    T.ToTensor(),
    T.Resize((560, 560)),
    T.Normalize(mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225])
])

# Load GT annotations
gt_path = "/content/rf-detr/data/visdrone/valid/_annotations.coco.json"
coco_gt = COCO(gt_path)
image_ids = coco_gt.getImgIds()

coco_gt = COCO(gt_path)
image_ids = coco_gt.getImgIds()

results = []

for image_id in tqdm(image_ids, desc="Running inference"):
    image_info = coco_gt.loadImgs(image_id)[0]
    file_name = image_info['file_name']
    img_path = os.path.join(test_images_dir, file_name)

    if not os.path.exists(img_path):
        print(f"Image {file_name} not found.")
        continue

    img = Image.open(img_path).convert("RGB")
    orig_w, orig_h = img.size
    input_tensor = transform(img).unsqueeze(0).cuda()

    # Run prediction
    with torch.no_grad():
        detections = detector.predict(img)

    # Extract bounding boxes, scores, and class_ids
    boxes = detections.xyxy
    scores = detections.confidence
    labels = detections.class_id

    # Iterate through detections and format for COCO evaluation
    for box, score, label in zip(boxes, scores, labels):
        x1, y1, x2, y2 = box.tolist()
        bbox_w = x2 - x1
        bbox_h = y2 - y1
        results.append({
            "image_id": image_id,
            "category_id": int(label) + 1,
            "bbox": [x1, y1, bbox_w, bbox_h],
            "score": float(score)
        })

output_json_path = "/content/rf-detr/output/predictions.json"
with open(output_json_path, "w") as f:
    json.dump(results, f, indent=4)

print(f"Predictions saved to {output_json_path}")

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

coco_dt = coco_gt.loadRes(output_json_path)
coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')
coco_eval.evaluate()
coco_eval.accumulate()
coco_eval.summarize()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

num_classes = len(class_names)
conf_matrix = np.zeros((num_classes, num_classes), dtype=int)

# Build map from image_id to predictions
from collections import defaultdict
preds_by_image = defaultdict(list)
for pred in results:
    preds_by_image[pred["image_id"]].append(pred)

# Loop over ground truth annotations and match
for img_id in tqdm(coco_gt.getImgIds(), desc="Building confusion matrix"):
    gt_annots = coco_gt.loadAnns(coco_gt.getAnnIds(imgIds=[img_id]))
    pred_annots = preds_by_image.get(img_id, [])
    matched_preds = set()

    for gt in gt_annots:
        gt_box = gt["bbox"]
        gt_cat = gt["category_id"]

        best_iou = 0
        best_pred = None
        best_pred_idx = 0

        for idx, pred in enumerate(pred_annots):
            if idx in matched_preds:
                continue

            pred_box = pred["bbox"]
            pred_cat = pred["category_id"] - 1


            # Compute IoU
            x1 = max(gt_box[0], pred_box[0])
            y1 = max(gt_box[1], pred_box[1])
            x2 = min(gt_box[0] + gt_box[2], pred_box[0] + pred_box[2])
            y2 = min(gt_box[1] + gt_box[3], pred_box[1] + pred_box[3])

            inter_area = max(0, x2 - x1) * max(0, y2 - y1)
            gt_area = gt_box[2] * gt_box[3]
            pred_area = pred_box[2] * pred_box[3]
            union_area = gt_area + pred_area - inter_area
            iou = inter_area / union_area if union_area > 0 else 0

            if iou > best_iou:
                best_iou = iou
                best_pred = pred_cat
                best_pred_idx = idx

        if best_iou >= 0.5 and best_pred is not None:
            if 0 <= gt_cat < num_classes and 0 <= best_pred < num_classes:
              conf_matrix[gt_cat][best_pred] += 1
              matched_preds.add(best_pred_idx)
        else:
            pass

plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", xticklabels=class_names, yticklabels=class_names, cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Ground Truth")
plt.title("Confusion Matrix")
plt.show()

TP = np.diag(conf_matrix)
FP = np.sum(conf_matrix, axis=0) - TP
FN = np.sum(conf_matrix, axis=1) - TP

precision = TP / (TP + FP + 1e-6)
recall = TP / (TP + FN + 1e-6)

for i, cls in enumerate(class_names):
    print(f"{cls}: Precision: {precision[i]:.2f}, Recall: {recall[i]:.2f}")

overall_accuracy = np.sum(TP) / (np.sum(TP) + np.sum(FP) + np.sum(FN) + 1e-6)

print(f"\nOverall Accuracy: {overall_accuracy:.2f}")